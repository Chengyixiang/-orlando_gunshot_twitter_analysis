## Step 1: Set up twitter oauth and load packages ###
library(twitteR)

## Step 2: scrawl large data ##
usa_cities <- read.csv("largest_cities_usa.csv")
S <- 200 #radius in miles 
N <- 10000
lats <- usa_cities$latitidue
lons <- usa_cities$longitude
cityName <- usa_cities$Largest.city

searchFunction <- function(i,kw){
  test<-searchTwitter(kw,
                      lang="en",
                      n=N,
                      since='2016-06-12 12:00:00', 
                      until='2016-06-14 23:59:59',
                      geocode=paste(lats[i],lons[i],paste0(S,"mi"),sep=","))
  
  y <- NA
  try_error = tryCatch(twListToDF(test), error=function(e) e)
  if (!inherits(try_error, "error")){
    y <- twListToDF(test)
  }
  return(y)
}

orlando_time_df <-data.frame()  

# create a loop 
for (i in 1:50){
  orlando_time <- searchFunction(i,kw=c('#Orlando','#OrlandoShooting','#OrlandoUnited','#TwoMenKissing','#LoveIsLove'))
  if (!is.na(orlando_time)){
    orlando_time$city<-i
    orlando_time_df<-rbind(orlando_time_df,orlando_time)
  }
  print (i)
}

write.csv(orlando_time_df,file = 'rawdata.csv')

### Step 3: Calculate Sentiment Score with Dictionary ###
orlando_sentimentscore <- read.csv("rawdata.csv")

install.packages("tm")
install.packages("SnowballC")
library(tm)
library(stringr)

# dictionary from https://github.com/jeffreybreen/twitter-sentiment-analysis-tutorial-201107/tree/master/data/opinion-lexicon-English
# citation:   Bing Liu, Minqing Hu and Junsheng Cheng. "Opinion Observer: Analyzing and Comparing Opinions on the Web." Proceedings of the 14th International World Wide Web conference (WWW-2005), May 10-14, 2005, Chiba, Japan.
positives<- readLines("positive_words.txt") 
negatives <- readLines("negative_words.txt")

# we firstly stem our dictionary 
pos <- stemDocument(positives, language = "english")
pos <- pos[!duplicated(pos)]
neg <- stemDocument(negatives, language = "english")
neg <- neg[!duplicated(neg)]

# clean up the text and just leave the text
clean.text <- function(some_txt) {
  some_txt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", some_txt)  # remove retweet entities
  some_txt = gsub("@\\w+", "", some_txt) # remove at people
  some_txt = gsub("[[:punct:]]", "", some_txt) # remove punctuation
  some_txt = gsub("[[:digit:]]", "", some_txt) # remove numbers
  some_txt = gsub("http\\w+", "", some_txt) # remove html links
  some_txt = gsub("[ \t]{2,}", "", some_txt) # remove unnecessary spaces
  some_txt = gsub("^\\s+|\\s+$", "", some_txt)
  # define "tolower error handling" function to keep the cleaning function running from being distrubed by NAs
  try.error = function(x)
  {
    # create missing value
    y = NA
    # tryCatch error
    try_error = tryCatch(tolower(x), error=function(e) e)
    # if not an error
    if (!inherits(try_error, "error"))
      y = tolower(x)
    # result
    return(y)
  }
  # lower case using try.error with sapply 
  some_txt = sapply(some_txt, try.error)
  names(some_txt) = NULL
  return (some_txt)
}

# improve by stemming and nagation, write a stemming function 
sentScore_stem <- function(tweet){
  words <- str_split(tweet, "[[:space:]]")[[1]]
  words <- stemDocument(words) 
  # stemDocument() is a function to stem words in a text document using Porter's stemming algorithm 
  positive.matches <- match(words, pos) 
  negative.matches <- match(words, neg)
  # rm NAs
  positive_matches <- !is.na(positive.matches)
  negative_matches <- !is.na(negative.matches)
  score <- sum(positive_matches) - sum(negative_matches)
  
  if (length(grep("not|n't|no|never|nor|against",tweet))>0){  # improve by nagation, and improve the accuracy of our scoring 
    score = -score                                            # nagation means add some negative words to let R recognize them as negative score
  }
  return (score)
}

# apply the precedent 2 functions to data sets
tweets <- orlando_sentimentscore$text
orlando_CleanTweets <- clean.text(tweets) 

scores <- sapply(orlando_CleanTweets,sentScore_stem) 
orlando_sentimentscore$sentiment <- scores
write.csv(orlando_sentimentscore,file = "orlando_sentimentscores.csv")

# ### Step 4: Polarity and Emotions ### not used this time
# install.packages("https://cran.r-project.org/src/contrib/Archive/Rstem/Rstem_0.4-1.tar.gz",repos = NULL)
# install.packages("https://cran.r-project.org/src/contrib/Archive/sentiment/sentiment_0.2.tar.gz", repos = NULL)
# library('sentiment')
# class_pol <- classify_polarity(tweets,algorithm="bayes")
# orlando_50000 <- cbind(orlando_50000,class_pol)
# 
# class_emo <- classify_emotion(orlando_CleanTweets, algorithm = "bayes")
# orlando_50000 <- cbind(orlando_50000,class_emo)
# 
# write.csv(orlando_50000,file = 'orlando_50000.csv')

### Step 5: create a word cloud and export a termDocument Matrix###
library("tm")
library("SnowballC")
library("wordcloud")
orlando_wordcloud <- read.csv('unique_tweets.csv')

save_tolower <- function(x) {
  return(chartr("ABCDEFGHIJKLMNOPQRSTUVWXYZ",
                "abcdefghijklmnopqrstuvwxyz", x))
}

Preprocessing <- function(sT){
  r_stats_text <- sapply(sT,function(row) iconv(row, "latin1", "ASCII", sub=""))
  #create corpus
  r_stats_text_corpus <- Corpus(VectorSource(r_stats_text))
  #clean up
  r_stats_text_corpus <- tm_map(r_stats_text_corpus, save_tolower)  ### Convert to lower case
  r_stats_text_corpus <- tm_map(r_stats_text_corpus, removePunctuation)   ### remove punctuation
  r_stats_text_corpus <- tm_map(r_stats_text_corpus, function(x)removeWords(x,stopwords("english")))  #### remove stopwords
  r_stats_text_corpus <- tm_map(r_stats_text_corpus, PlainTextDocument)  
  return(r_stats_text_corpus)
}

# in order to process the error problem
clean.text <- function(some_txt) {
  some_txt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", some_txt)  # remove retweet entities
  some_txt = gsub("@\\w+", "", some_txt) # remove at people
  some_txt = gsub("[[:punct:]]", "", some_txt) # remove punctuation
  some_txt = gsub("[[:digit:]]", "", some_txt) # remove numbers
  some_txt = gsub("http\\w+", "", some_txt) # remove html links
  some_txt = gsub("[ \t]{2,}", "", some_txt) # remove unnecessary spaces
  some_txt = gsub("^\\s+|\\s+$", "", some_txt)
  some_txt = gsub("&amp","",some_txt)
  some_txt = gsub("#\\w+","",some_txt) #remove hashtags
  some_txt = gsub("ing|ed|es","",some_txt) # somehow, stem manually, since corpus has bugs that i can not fix
  some_txt = stemDocument(some_txt)  # stem is necesssary to narrow down our terms. however, the stemming function in DTM can not process large files, so we process it here, in the cleaning phrase
   # define "tolower error handling" function to keep the cleaning function running from being distrubed by NAs
  try.error = function(x)
  {
    # create missing value
    y = NA
    # tryCatch error
    try_error = tryCatch(tolower(x), error=function(e) e)
    # if not an error
    if (!inherits(try_error, "error"))
      y = tolower(x)
    # result
    return(y)
  }
  # lower case using try.error with sapply 
  some_txt = sapply(some_txt, try.error)
  names(some_txt) = NULL
  return (some_txt)
}

# clean the text 
orlando_wordcloud_clean <- clean.text(orlando_wordcloud$text) # until now, it is all good
# create a corpus to do dtm
orlando_corpus4 <- Preprocessing(orlando_wordcloud_clean) 
orlando_dtm <- DocumentTermMatrix(orlando_corpus4) # until now, there is no error
save(orlando_dtm, file = "orlando_dtm.R")

### THIS IS THE FUCKING KEY POINT! STEMMING IS ALSO THE KEY POINT!!! so that the as.matrix function could be used on the big file which is not that big now.
orlando_dtm_1 <- removeSparseTerms(orlando_dtm, 0.99)
rowTotals <- apply(orlando_dtm_1 , 1, sum) #Find the sum of words in each Document
orlando_dtm_1 <- orlando_dtm_1[rowTotals> 0, ] #Remove all docs without words

orlando_matrix <- as.matrix(orlando_dtm_1)

freq <- colSums(as.matrix(orlando_dtm_1))
freq <- sort(freq,decreasing = T)
orland_freq <- data.frame(word=names(freq),freq = freq)
write.csv(orland_freq,file = "orlando_freq.csv") # then we can use tableau to do the word cloud

### directly create a word cloud by myself, so fucking ugly
wordcloud(orlando_corpus4, 
          scale=c(5,.4), max.words=100,
          random.order=F, rot.per=.2,
          colors=brewer.pal(4, "Dark2"))   











